{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7982f5da",
   "metadata": {},
   "source": [
    "# Natural Language Processing with Text Classification\n",
    "\n",
    "This notebook demonstrates various NLP techniques using a text classification task. We'll cover:\n",
    "1. Text preprocessing and cleaning\n",
    "2. Feature extraction (Bag of Words, TF-IDF)\n",
    "3. Word embeddings (Word2Vec)\n",
    "4. Model training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a85714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9935ea9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Load the 20 Newsgroups dataset\n",
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "newsgroups = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'text': newsgroups.data,\n",
    "    'target': newsgroups.target,\n",
    "    'category': [newsgroups.target_names[i] for i in newsgroups.target]\n",
    "})\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Number of documents: {len(df)}\")\n",
    "print(f\"Number of categories: {len(categories)}\")\n",
    "print(\"\\nCategory distribution:\")\n",
    "print(df['category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0fd418",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29243c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the text\n",
    "df['processed_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['processed_text'],\n",
    "    df['target'],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df['target']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5ecdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words representation\n",
    "bow_vectorizer = CountVectorizer(max_features=1000)\n",
    "X_train_bow = bow_vectorizer.fit_transform(X_train)\n",
    "X_test_bow = bow_vectorizer.transform(X_test)\n",
    "\n",
    "# Train a Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train_bow, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = nb_classifier.predict(X_test_bow)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Naive Bayes with Bag of Words:\")\n",
    "print(classification_report(y_test, y_pred, target_names=categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835f0d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF representation\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Train a Logistic Regression classifier\n",
    "lr_classifier = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = lr_classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Logistic Regression with TF-IDF:\")\n",
    "print(classification_report(y_test, y_pred, target_names=categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01989180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec embeddings\n",
    "# Prepare text for Word2Vec\n",
    "sentences = [text.split() for text in df['processed_text']]\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(\n",
    "    sentences,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    workers=4,\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "# Function to get document vectors\n",
    "def get_document_vector(text, model):\n",
    "    words = text.split()\n",
    "    vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    if not vectors:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "# Get document vectors\n",
    "X_train_w2v = np.array([get_document_vector(text, word2vec_model) for text in X_train])\n",
    "X_test_w2v = np.array([get_document_vector(text, word2vec_model) for text in X_test])\n",
    "\n",
    "# Train a Logistic Regression classifier\n",
    "lr_w2v = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_w2v.fit(X_train_w2v, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = lr_w2v.predict(X_test_w2v)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Logistic Regression with Word2Vec:\")\n",
    "print(classification_report(y_test, y_pred, target_names=categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faf1795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix for the best performing model\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    confusion_matrix(y_test, y_pred),\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=categories,\n",
    "    yticklabels=categories\n",
    ")\n",
    "plt.title('Confusion Matrix - Logistic Regression with Word2Vec')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c83d686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize word embeddings using PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Get word vectors\n",
    "words = list(word2vec_model.wv.key_to_index.keys())[:100]\n",
    "word_vectors = np.array([word2vec_model.wv[word] for word in words])\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "word_vectors_2d = pca.fit_transform(word_vectors)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.scatter(word_vectors_2d[:, 0], word_vectors_2d[:, 1])\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, (word_vectors_2d[i, 0], word_vectors_2d[i, 1]))\n",
    "plt.title('Word Embeddings Visualization (PCA)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f149a98",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we explored various NLP techniques for text classification:\n",
    "\n",
    "1. **Text Preprocessing**:\n",
    "   - Implemented text cleaning, tokenization, and stemming\n",
    "   - Removed stopwords and special characters\n",
    "   - Normalized text to lowercase\n",
    "\n",
    "2. **Feature Extraction**:\n",
    "   - Bag of Words representation\n",
    "   - TF-IDF vectorization\n",
    "   - Word2Vec embeddings\n",
    "\n",
    "3. **Model Performance**:\n",
    "   - Logistic Regression with TF-IDF performed well\n",
    "   - Word2Vec embeddings provided semantic understanding\n",
    "   - Naive Bayes with Bag of Words was a good baseline\n",
    "\n",
    "4. **Key Insights**:\n",
    "   - Different feature extraction methods have different strengths\n",
    "   - Word embeddings capture semantic relationships\n",
    "   - Text preprocessing is crucial for good performance\n",
    "\n",
    "This notebook serves as a good starting point for understanding NLP techniques and their application to text classification problems. "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
